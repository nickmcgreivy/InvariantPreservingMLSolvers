{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31bc80e8",
   "metadata": {},
   "source": [
    "In this Jupyter notebook, we will train a machine learned FV solver and machine learned DG solver to solve the 1D advection equation at reduced resolution. Our objective is to study the frequency of instability, and to demonstrate that global stabilization eliminates this instability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf4d4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup paths\n",
    "import sys\n",
    "basedir = '/Users/nickm/thesis/icml2023paper/1d_advection'\n",
    "readwritedir = '/Users/nickm/thesis/icml2023paper/1d_advection'\n",
    "\n",
    "sys.path.append('{}/core'.format(basedir))\n",
    "sys.path.append('{}/simulate'.format(basedir))\n",
    "sys.path.append('{}/ml'.format(basedir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240d087b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import external packages\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as onp\n",
    "from jax import config, vmap\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "import xarray\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1499b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import internal packages\n",
    "from initialconditions import get_a0, get_initial_condition_fn, get_a\n",
    "from simparams import CoreParams, CoreParamsDG, SimulationParams\n",
    "from legendre import generate_legendre\n",
    "from simulations import AdvectionFVSim, AdvectionDGSim\n",
    "from trajectory import get_trajectory_fn, get_inner_fn\n",
    "from trainingutils import save_training_data\n",
    "from mlparams import TrainingParams, StencilParams\n",
    "from model import LearnedStencil\n",
    "from trainingutils import (get_loss_fn, get_batch_fn, get_idx_gen, train_model, \n",
    "                           compute_losses_no_model, init_params, save_training_params, load_training_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614be22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "def plot_fv(a, core_params, color=\"blue\"):\n",
    "    plot_dg(a[...,None], core_params, color=color)\n",
    "    \n",
    "def plot_fv_trajectory(trajectory, core_params, t_inner, color='blue'):\n",
    "    plot_dg_trajectory(trajectory[...,None], core_params, t_inner, color=color)\n",
    "    \n",
    "def plot_dg(a, core_params, color='blue'):\n",
    "    if core_params.order is None:\n",
    "        p = 1\n",
    "    else:\n",
    "        p = core_params.order + 1\n",
    "    def evalf(x, a, j, dx, leg_poly):\n",
    "        x_j = dx * (0.5 + j)\n",
    "        xi = (x - x_j) / (0.5 * dx)\n",
    "        vmap_polyval = vmap(jnp.polyval, (0, None), -1)\n",
    "        poly_eval = vmap_polyval(leg_poly, xi)  # nx, p array\n",
    "        return jnp.sum(poly_eval * a, axis=-1)\n",
    "\n",
    "    NPLOT = [2,2,5,7][p-1]\n",
    "    nx = a.shape[0]\n",
    "    dx = core_params.Lx / nx\n",
    "    xjs = jnp.arange(nx) * core_params.Lx / nx\n",
    "    xs = xjs[None, :] + jnp.linspace(0.0, dx, NPLOT)[:, None]\n",
    "    vmap_eval = vmap(evalf, (1, 0, 0, None, None), 1)\n",
    "\n",
    "    a_plot = vmap_eval(xs, a, jnp.arange(nx), dx, generate_legendre(p))\n",
    "    a_plot = a_plot.T.reshape(-1)\n",
    "    xs = xs.T.reshape(-1)\n",
    "    coords = {('x'): xs}\n",
    "    data = xarray.DataArray(a_plot, coords=coords)\n",
    "    data.plot(color=color)\n",
    "\n",
    "def plot_dg_trajectory(trajectory, core_params, t_inner, color='blue'):\n",
    "    if core_params.order is None:\n",
    "        p = 1\n",
    "    else:\n",
    "        p = core_params.order + 1\n",
    "    NPLOT = [2,2,5,7][p-1]\n",
    "    nx = trajectory.shape[1]\n",
    "    dx = core_params.Lx / nx\n",
    "    xjs = jnp.arange(nx) * core_params.Lx / nx\n",
    "    xs = xjs[None, :] + jnp.linspace(0.0, dx, NPLOT)[:, None]\n",
    "    \n",
    "    def get_plot_repr(a):\n",
    "        def evalf(x, a, j, dx, leg_poly):\n",
    "            x_j = dx * (0.5 + j)\n",
    "            xi = (x - x_j) / (0.5 * dx)\n",
    "            vmap_polyval = vmap(jnp.polyval, (0, None), -1)\n",
    "            poly_eval = vmap_polyval(leg_poly, xi)  # nx, p array\n",
    "            return jnp.sum(poly_eval * a, axis=-1)\n",
    "\n",
    "        vmap_eval = vmap(evalf, (1, 0, 0, None, None), 1)\n",
    "        return vmap_eval(xs, a, jnp.arange(nx), dx, generate_legendre(p)).T\n",
    "\n",
    "    get_trajectory_plot_repr = vmap(get_plot_repr)\n",
    "    trajectory_plot = get_trajectory_plot_repr(trajectory)\n",
    "\n",
    "    outer_steps = trajectory.shape[0]\n",
    "    \n",
    "    trajectory_plot = trajectory_plot.reshape(outer_steps, -1)\n",
    "    xs = xs.T.reshape(-1)\n",
    "    coords = {\n",
    "        'x': xs,\n",
    "        'time': t_inner * jnp.arange(outer_steps)\n",
    "    }\n",
    "    xarray.DataArray(trajectory_plot, dims=[\"time\", \"x\"], coords=coords).plot(\n",
    "        col='time', col_wrap=5, color=color)\n",
    "    \n",
    "\n",
    "def get_core_params(order, flux='upwind'):\n",
    "    Lx = 1.0\n",
    "    if order == 0:\n",
    "        return CoreParams(Lx, flux)\n",
    "    else:\n",
    "        return CoreParamsDG(Lx, flux, order)\n",
    "\n",
    "def get_sim_params(name = \"test\", cfl_safety=0.3, rk='ssp_rk3'):\n",
    "    return SimulationParams(name, basedir, readwritedir, cfl_safety, rk)\n",
    "\n",
    "def get_training_params(n_data, train_id=\"test\", batch_size=4, learning_rate=1e-3, num_epochs = 10, optimizer='sgd'):\n",
    "    return TrainingParams(n_data, num_epochs, train_id, batch_size, learning_rate, optimizer)\n",
    "\n",
    "def get_stencil_params(kernel_size = 3, kernel_out = 4, stencil_width=4, depth = 3, width = 16):\n",
    "    return StencilParams(kernel_size, kernel_out, stencil_width, depth, width)\n",
    "\n",
    "def get_model(core_params, stencil_params):\n",
    "    if core_params.order is None:\n",
    "        p = 1\n",
    "    else:\n",
    "        p = core_params.order + 1\n",
    "    features = [stencil_params.width for _ in range(stencil_params.depth - 1)]\n",
    "    return LearnedStencil(features, stencil_params.kernel_size, stencil_params.kernel_out, stencil_params.stencil_width, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfb2989",
   "metadata": {},
   "source": [
    "### Finite Volume\n",
    "\n",
    "##### Training Loop\n",
    "\n",
    "First, we will generate the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907c073d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training hyperparameters\n",
    "init_description = 'sum_sin'\n",
    "kwargs_init = {'min_num_modes': 1, 'max_num_modes': 6, 'min_k': 1, 'max_k': 4, 'amplitude_max': 1.0}\n",
    "kwargs_sim = {'name' : \"test\", 'cfl_safety' : 0.2, 'rk' : 'ssp_rk3'}\n",
    "kwargs_train_FV = {'train_id': \"test\", 'batch_size' : 8, 'optimizer': 'adam', 'learning_rate' : 1e-3,  'num_epochs' : 50}\n",
    "#kwargs_train_DG = {'train_id': \"test\", 'batch_size' : 8, 'optimizer': 'adam', 'learning_rate' : 1e-5, 'num_epochs' : 10}\n",
    "kwargs_stencil = {'kernel_size' : 3, 'kernel_out' : 4, 'stencil_width' : 4, 'depth' : 3, 'width' : 16}\n",
    "n_runs = 50\n",
    "t_inner_train = 0.02\n",
    "outer_steps_train = int(1.0/t_inner_train)\n",
    "fv_flux_baseline = 'muscl' # learning a correction to the MUSCL scheme\n",
    "nx_exact = 256\n",
    "nxs = [8, 16, 32, 64]\n",
    "plot_colors = ['blue', 'green', 'orange', 'purple']\n",
    "key = jax.random.PRNGKey(12)\n",
    "\n",
    "# setup\n",
    "core_params = get_core_params(0, flux=fv_flux_baseline)\n",
    "sim_params = get_sim_params(**kwargs_sim)\n",
    "n_data = n_runs * outer_steps_train\n",
    "training_params = get_training_params(n_data, **kwargs_train_FV)\n",
    "stencil_params = get_stencil_params(**kwargs_stencil)\n",
    "sim = AdvectionFVSim(core_params, sim_params)\n",
    "init_fn = lambda key: get_initial_condition_fn(core_params, init_description, key=key, **kwargs_init)\n",
    "model = get_model(core_params, stencil_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1592f3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save training data\n",
    "save_training_data(key, init_fn, core_params, sim_params, sim, t_inner_train, outer_steps_train, n_runs, nx_exact, nxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d261c8c2",
   "metadata": {},
   "source": [
    "Next, we initialize the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e70430c",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(42)\n",
    "i_params = init_params(key, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6244e5b8",
   "metadata": {},
   "source": [
    "Next, we run a training loop for each value of nx. The learning rate undergoes a prespecified decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76667430",
   "metadata": {},
   "outputs": [],
   "source": [
    "for nx in nxs:\n",
    "    print(nx)\n",
    "    idx_fn = lambda key: get_idx_gen(key, training_params)\n",
    "    batch_fn = get_batch_fn(core_params, sim_params, training_params, nx)\n",
    "    loss_fn = get_loss_fn(model, core_params)\n",
    "    losses, params = train_model(model, i_params, training_params, key, idx_fn, batch_fn, loss_fn)\n",
    "    save_training_params(nx, sim_params, training_params, params, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431b6dcd",
   "metadata": {},
   "source": [
    "Next, we load and plot the losses for each nx to check that the simulation trained properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381a6f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "for nx in nxs:\n",
    "    losses, _ = load_training_params(nx, sim_params, training_params, model)\n",
    "    plt.plot(losses, label=nx)\n",
    "plt.ylim([0,1])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cba0ca",
   "metadata": {},
   "source": [
    "Next, we plot the accuracy of the trained model on a few simple test cases to qualitatively evaluate the success of the training. We will eventually quantify the accuracy of the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7aa4c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a key that gives something nice\n",
    "key = jax.random.PRNGKey(18)\n",
    "\n",
    "for i, nx in enumerate(nxs):\n",
    "    print(\"nx is {}\".format(nx))\n",
    "    _, params = load_training_params(nx, sim_params, training_params, model)\n",
    "    \n",
    "    f_init = get_initial_condition_fn(core_params, init_description, key=key, **kwargs_init)\n",
    "    a0 = get_a0(f_init, core_params, nx)\n",
    "    t_inner = 1.0\n",
    "    outer_steps = 5\n",
    "    # with params\n",
    "    sim_model = AdvectionFVSim(core_params, sim_params, model=model, params=params)\n",
    "    inner_fn_model = get_inner_fn(sim_model.step_fn, sim.dt_fn, t_inner)\n",
    "    trajectory_fn_model = get_trajectory_fn(inner_fn_model, outer_steps)\n",
    "    trajectory = trajectory_fn_model(a0)\n",
    "    plot_fv_trajectory(trajectory, core_params, t_inner, color = plot_colors[i])\n",
    "\n",
    "    # without params\n",
    "    inner_fn = get_inner_fn(sim.step_fn, sim.dt_fn, t_inner)\n",
    "    trajectory_fn_= get_trajectory_fn(inner_fn, outer_steps)\n",
    "    trajectory = trajectory_fn(a0)\n",
    "    plot_fv_trajectory(trajectory, core_params, t_inner, color = 'red')\n",
    "    \n",
    "    #exact_trajectory = a0[None, ...] * jnp.ones(trajectory.shape[0])[:, None]\n",
    "    #plot_fv_trajectory(exact_trajectory, core_params, t_inner, color='red')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1eb0e8",
   "metadata": {},
   "source": [
    "We see from above that the baseline (red) has a large amount of numerical diffusion for small number of gridpoints, while is more accurate for more gridpoints. We also see that the machine learned model learns to accurately evolve the solution for nx > 8. So far, so good. Let's now look at a different initial condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3ae205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a key that gives something nice\n",
    "key = jax.random.PRNGKey(20)\n",
    "\n",
    "for i, nx in enumerate(nxs):\n",
    "    print(\"nx is {}\".format(nx))\n",
    "    _, params = load_training_params(nx, sim_params, training_params, model)\n",
    "    \n",
    "    f_init = get_initial_condition_fn(core_params, init_description, key=key, **kwargs_init)\n",
    "    a0 = get_a0(f_init, core_params, nx)\n",
    "    t_inner = 1.0\n",
    "    outer_steps = 5\n",
    "    # with params\n",
    "    \n",
    "    sim_model = AdvectionFVSim(core_params, sim_params, model=model, params=params)\n",
    "    inner_fn_model = get_inner_fn(sim_model.step_fn, sim_model.dt_fn, t_inner)\n",
    "    trajectory_fn_model = get_trajectory_fn(inner_fn_model, outer_steps)\n",
    "    trajectory = trajectory_fn_model(a0)\n",
    "    plot_fv_trajectory(trajectory, core_params, t_inner, color = plot_colors[i])\n",
    "    \n",
    "\n",
    "    # without params\n",
    "    inner_fn = get_inner_fn(sim.step_fn, sim.dt_fn, t_inner)\n",
    "    trajectory_fn = get_trajectory_fn(inner_fn, outer_steps)\n",
    "    \n",
    "    trajectory = trajectory_fn(a0)\n",
    "    plot_fv_trajectory(trajectory, core_params, t_inner, color = 'red')\n",
    "    \n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5900a111",
   "metadata": {},
   "source": [
    "Oh no! We can see that for nx=8 and nx=32, the solution goes unstable between 1.0 < t < 2.0 and 0.0 < t < 1.0 respectively.\n",
    "\n",
    "This is not good. Even though the machine learned PDE solver gives accurate solution for certain initial conditions, the solution blows up unexpectedly for certain initial conditions. Let's next ask: how frequently does an instability arise?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53686756",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "\n",
    "for nx in nxs:\n",
    "    \n",
    "    key = jax.random.PRNGKey(10) # new key, same initial key for each nx\n",
    "    _, params = load_training_params(nx, sim_params, training_params, model)\n",
    "    t_inner = 1.0\n",
    "    outer_steps = 5\n",
    "    sim_model = AdvectionFVSim(core_params, sim_params, model=model, params=params)\n",
    "    inner_fn_model = get_inner_fn(sim_model.step_fn, sim_model.dt_fn, t_inner)\n",
    "    trajectory_fn_model = get_trajectory_fn(inner_fn_model, outer_steps)\n",
    "    \n",
    "    inner_fn = get_inner_fn(sim.step_fn, sim.dt_fn, t_inner)\n",
    "    trajectory_fn = get_trajectory_fn(inner_fn, outer_steps)\n",
    "    \n",
    "    num_nan = 0\n",
    "    \n",
    "    num_nan_baseline = 0\n",
    "    \n",
    "    for n in range(N):\n",
    "        key, _ = jax.random.split(key)\n",
    "        \n",
    "        f_init = get_initial_condition_fn(core_params, init_description, key=key, **kwargs_init)\n",
    "        a0 = get_a0(f_init, core_params, nx)\n",
    "        trajectory_model = trajectory_fn_model(a0)\n",
    "        \n",
    "        \n",
    "        num_nan += jnp.isnan(trajectory_model[-1]).any()\n",
    "        \n",
    "        trajectory = trajectory_fn(a0)\n",
    "        \n",
    "        num_nan_baseline += jnp.isnan(trajectory[-1]).any()\n",
    "        \n",
    "        \n",
    "    print(\"nx is {}, num_nan is {} out of {}\".format(nx, num_nan, N))\n",
    "    print(\"How many of baseline go unstable: {}\".format(num_nan_baseline))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2831054a",
   "metadata": {},
   "source": [
    "So we see that a large percentage of the simulations go unstable. We could use various tips and tricks to decrease the number of simulations that go unstable, such as to increase the size of the training set or the duration of training or use a different loss function. But we are interested in something else entirely: eliminating the ability of the solution to go unstable. What happens when we set global stabilization to True?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268fa07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 50\n",
    "\n",
    "for nx in nxs:\n",
    "    \n",
    "    key = jax.random.PRNGKey(10)\n",
    "    _, params = load_training_params(nx, sim_params, training_params, model)\n",
    "    t_inner = 1.0\n",
    "    outer_steps = 5\n",
    "    sim_model = AdvectionFVSim(core_params, sim_params, global_stabilization = True, model=model, params=params)\n",
    "    inner_fn_model = get_inner_fn(sim_model.step_fn, sim_model.dt_fn, t_inner)\n",
    "    trajectory_fn_model = get_trajectory_fn(inner_fn_model, outer_steps)\n",
    "    \n",
    "    num_nan = 0\n",
    "    \n",
    "    for n in range(N):\n",
    "        f_init = get_initial_condition_fn(core_params, init_description, key=key, **kwargs_init)\n",
    "        a0 = get_a0(f_init, core_params, nx)\n",
    "        trajectory_model = trajectory_fn_model(a0)\n",
    "        num_nan += jnp.isnan(trajectory_model[-1]).any()\n",
    "        key, _ = jax.random.split(key)\n",
    "        \n",
    "    print(\"nx is {}, num_nan is {} out of {}\".format(nx, num_nan, N))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fe8f2b",
   "metadata": {},
   "source": [
    "We see that, as expected, the global stabilization method eliminates NaNs from the final solution. This is a demonstration of our claim that the solution is provably stable (in the time-continuous limit).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc83e37",
   "metadata": {},
   "source": [
    "### Demonstrate Stabilization of Solution\n",
    "\n",
    "Now we will plot (a) the exact solution (b) the ML solution (c) the stabilized ML solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a6edff",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx = 32\n",
    "key = jax.random.PRNGKey(10) # new key, same initial key for each nx\n",
    "\n",
    "t_inner = 0.1\n",
    "outer_steps = 5\n",
    "_, params = load_training_params(nx, sim_params, training_params, model)\n",
    "f_init = get_initial_condition_fn(core_params, init_description, key=key, **kwargs_init)\n",
    "a0 = get_a0(f_init, core_params, nx)\n",
    "\n",
    "# ML model\n",
    "sim_model = AdvectionFVSim(core_params, sim_params, model=model, params=params)\n",
    "inner_fn_model = get_inner_fn(sim_model.step_fn, sim_model.dt_fn, t_inner)\n",
    "trajectory_fn_model = get_trajectory_fn(inner_fn_model, outer_steps)\n",
    "trajectory_model = trajectory_fn_model(a0)\n",
    "# ML global stabilized model\n",
    "sim_model_gs = AdvectionFVSim(core_params, sim_params, global_stabilization=True, model=model, params=params)\n",
    "inner_fn_model_gs = get_inner_fn(sim_model_gs.step_fn, sim_model_gs.dt_fn, t_inner)\n",
    "trajectory_fn_model_gs = get_trajectory_fn(inner_fn_model_gs, outer_steps)\n",
    "trajectory_model_gs = trajectory_fn_model_gs(a0)\n",
    "# exact trajectory\n",
    "exact_trajectory = onp.zeros(trajectory_model.shape)\n",
    "for n in range(outer_steps):\n",
    "    t = n * t_inner\n",
    "    exact_trajectory[n] = get_a(f_init, t, core_params, nx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76cdc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = core_params.Lx\n",
    "\n",
    "def plot_subfig(\n",
    "    a, subfig, L, color=\"blue\", linewidth=0.5, linestyle=\"solid\", label=None\n",
    "):\n",
    "    def evalf(x, a, j, dx):\n",
    "        x_j = dx * (0.5 + j)\n",
    "        xi = (x - x_j) / (0.5 * dx)\n",
    "        vmap_polyval = vmap(jnp.polyval, (0, None), -1)\n",
    "        poly_eval = vmap_polyval(jnp.asarray([[1.]]), xi)\n",
    "        return jnp.sum(poly_eval * a, axis=-1)\n",
    "\n",
    "    nx = a.shape[0]\n",
    "    dx = L / nx\n",
    "    xjs = jnp.arange(nx) * L / nx\n",
    "    xs = xjs[None, :] + jnp.linspace(0.0, dx, 10)[:, None]\n",
    "    vmap_eval = vmap(evalf, (1, 0, 0, None), 1)\n",
    "    subfig.plot(\n",
    "        xs,\n",
    "        vmap_eval(xs, a, jnp.arange(nx), dx),\n",
    "        color=color,\n",
    "        linewidth=linewidth,\n",
    "        label=label,\n",
    "        linestyle=linestyle,\n",
    "    )\n",
    "    return\n",
    "\n",
    "Np = 4\n",
    "fig, axs = plt.subplots(1, Np, sharex=True, sharey=True, squeeze=True, figsize=(8,8/4))\n",
    "\n",
    "\n",
    "for j in range(Np):\n",
    "    plot_subfig(exact_trajectory[j], axs[j], L, color=\"grey\", label=\"Exact\\nsolution\", linewidth=0.75)\n",
    "    plot_subfig(trajectory_model[j], axs[j], L, color=\"#ff5555\", label=\"ML Solver\", linewidth=2.0)\n",
    "    plot_subfig(trajectory_model_gs[j], axs[j], L, color=\"#003366\", label=\"Stabilized\\nML Solver\", linewidth=2.0)\n",
    "    axs[j].plot(onp.zeros(len(exact_trajectory[j])), '--',  color=\"black\", linewidth=0.25)\n",
    "\n",
    "axs[0].set_xlim([0, 1])\n",
    "axs[0].set_ylim([-2.0, 2.0])\n",
    "\n",
    "axs[0].spines['left'].set_visible(False)\n",
    "axs[Np-1].spines['right'].set_visible(False)\n",
    "for j in range(Np):\n",
    "    axs[j].set_yticklabels([])\n",
    "    axs[j].set_xticklabels([])\n",
    "    axs[j].spines['top'].set_visible(False)\n",
    "    axs[j].spines['bottom'].set_visible(False)\n",
    "    axs[j].tick_params(bottom=False)\n",
    "    axs[j].tick_params(left=False)\n",
    "\n",
    "\n",
    "\n",
    "#plt.style.use('seaborn')\n",
    "#plt.rcParams.update(tex_fonts)\n",
    "\n",
    "\n",
    "\n",
    "props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "\n",
    "\"\"\"\n",
    "axs[0].text(0.4, 0.95, r'$\\frac{\\partial u}{\\partial t} + \\frac{\\partial u}{\\partial x} = 0$', transform=axs[0].transAxes, fontsize=10,\n",
    "        verticalalignment='top', bbox=props)\n",
    "# place a text box in upper left in axes coords\n",
    "axs[0].text(0.05, 0.95, \"$t=0.0$\", transform=axs[0].transAxes, fontsize=12,\n",
    "        verticalalignment='top')\n",
    "axs[1].text(0.05, 0.95, \"$t=0.1$\", transform=axs[1].transAxes, fontsize=12,\n",
    "        verticalalignment='top')\n",
    "axs[2].text(0.05, 0.95, \"$t=0.2$\", transform=axs[2].transAxes, fontsize=12,\n",
    "        verticalalignment='top')\n",
    "axs[3].text(0.05, 0.95, \"$t=0.3$\", transform=axs[3].transAxes, fontsize=12,\n",
    "        verticalalignment='top')\n",
    "\"\"\"\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "by_label = dict(zip(labels, handles))\n",
    "#fig.legend(by_label.values(), by_label.keys(),loc=(0.003,0.001), prop={'size': 9.5})\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a99aa32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
